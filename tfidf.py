# -*- coding: utf-8 -*-
"""tfidf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/0B4uSLhZ3w_ApZWRfcmRNaWp1NnlseUphaGI3TWNhUFJ3Y0I4
"""

from google.colab import drive
drive.mount('/content/drive')

cd 'drive/My Drive'

!unzip bbcsport-fulltext.zip

mv '/content/drive/My Drive/bbcsport' '/content/bbcsport'

stopwords_path  = '/content/stopwords.txt'
document_path = '/content/bbcsport/cricket/001.txt'
preprocesssed_file_path = '/content/preprocessed.txt'

#using nltk library
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.stem import PorterStemmer
import re
import os
nltk.download('punkt')
nltk.download('stopwords')
stemmer = PorterStemmer()

for folder in os.listdir('/content/bbcsport/'):
  if not os.path.isdir(os.path.join('/content/bbcsport',folder)):
      continue
  for file in os.listdir(os.path.join('/content/bbcsport',folder)):
    document_path = os.path.join(os.path.join('/content/bbcsport',folder),file)
    document_content = open(document_path,'r',encoding="utf8", errors='ignore')
    content = document_content.read()
    content = re.sub(r'[^a-zA-Z ]','',content)
    tokenizer = RegexpTokenizer(r'\w+')
    words = tokenizer.tokenize(content)
    preprocessed_content = [word.lower() for word in words if word.lower() not in stopwords.words('english')]
    preprocessed_content = ' '.join([stemmer.stem(word) for word in preprocessed_content])
    preprocesssed_file_path = os.path.join(os.path.join('/content/bbcsport',folder),'preprocess_'+file)
    open(preprocesssed_file_path,'w').write(preprocessed_content)
    os.remove(document_path)

import numpy as np
unique_words = set()
count_doc = 0
for folder in os.listdir('/content/bbcsport/'):
  if not os.path.isdir(os.path.join('/content/bbcsport',folder)):
      continue
  for file in os.listdir(os.path.join('/content/bbcsport',folder)):
    document_path = os.path.join(os.path.join('/content/bbcsport',folder),file)
    count_doc += 1
    for x in open(document_path,'r').read().split(' '):
      unique_words.add(x)
unique_words = sorted(list(unique_words))
print(len(unique_words))

word_ind = dict(list(zip(unique_words,range(len(unique_words)))))
count = np.zeros((count_doc,len(unique_words))).astype('int32')
doc_num = 0
for folder in os.listdir('/content/bbcsport/'):
  if not os.path.isdir(os.path.join('/content/bbcsport',folder)):
      continue
  for file in os.listdir(os.path.join('/content/bbcsport',folder)):
    document_path = os.path.join(os.path.join('/content/bbcsport',folder),file)
    for x in open(document_path,'r').read().split(' '):
      count[doc_num,word_ind[x]] += 1
    doc_num += 1
total_count_of_word = np.sum(count,axis=0)

print(count)

print(total_count_of_word)

#first 20  max
import matplotlib.pyplot as plt
data = list(zip(*sorted(list(enumerate(total_count_of_word)),key = lambda x:-x[1])))
num = 20
plt.bar(range(num),data[1][:num],tick_label=list(map(lambda x:unique_words[x],data[0][:num])))
plt.xticks(rotation='vertical')

print (idf)

from sklearn.metrics.pairwise import cosine_similarity

query = input('Enter query: ')
query = preprocess(tokenizer,query)
query_tf = np.zeros(len(unique_words))

for x in query.split(' '):
    if x in word_ind.keys():
        query_tf[word_ind[x]] += 1
    print(query)
    print(query_tf)
    query_tfidf = (query_tf*idf).reshape(1,len(query_tf))
    while True:
        similarity = cosine_similarity(tfidf,query_tfidf)
        similarity = similarity.reshape(len(similarity))
        similarity = sorted(list(enumerate(similarity)),key=lambda x:-x[1])
    for